{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnLyyfb0FGqucawuX9qyA4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cooolbabu/GoogleGemini101/blob/main/AzureDatabricks/BookstoreMedallion_BookstoreDB_OpenAI_T1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package installs"
      ],
      "metadata": {
        "id": "awVI93fo_PXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai\n",
        "%pip install PyGithub"
      ],
      "metadata": {
        "id": "SUckIMaorrfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cebd841-8b39-4e6a-8d80-8efa2d3d126a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.14.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.5.0)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.31.0)\n",
            "Requirement already satisfied: pyjwt[crypto]>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (4.10.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.0.7)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.2.14)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->PyGithub) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (2024.2.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->PyGithub) (1.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports, get keys, get llm client and set model to variable MODEL_NAME"
      ],
      "metadata": {
        "id": "xXANfThOTUJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "import re\n",
        "from openai import OpenAI\n",
        "from pprint import pprint\n",
        "from google.colab import userdata\n",
        "from github import Github\n",
        "\n",
        "# Get the OpenAI API key from Colab secrets\n",
        "github_token=userdata.get('Github_Token')\n",
        "openai_api_key=userdata.get('OPENAI_API_KEY')\n",
        "# Initialize a GitHub instance\n",
        "g = Github(github_token)\n",
        "\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "MODEL_NAME = \"gpt-3.5-turbo-1106\""
      ],
      "metadata": {
        "id": "HgzQz8VirhAw"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Github helper functions\n",
        "* read_file_as_string()\n",
        "* check_in_file(repo_name, file_path, file_content, content_tag, branch)"
      ],
      "metadata": {
        "id": "2TKZOKLeXN1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_as_string(file_path):\n",
        "  \"\"\"\n",
        "      Reads the file and return a string representation of the file contents\n",
        "\n",
        "      Parameters:\n",
        "          file_path (str): Filename including filepath\n",
        "  \"\"\"\n",
        "  try:\n",
        "      with open(file_path, 'r') as file:\n",
        "          file_contents = file.read()\n",
        "      return file_contents\n",
        "  except FileNotFoundError:\n",
        "      print(f\"File '{file_path}' not found.\")\n",
        "      return None\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "      return None\n",
        "\n",
        "def check_in_file(repo_name, file_path, file_content, content_tag, branch):\n",
        "    \"\"\"\n",
        "        Checks if a specific file exists in a GitHub repository and updates it with new content if it does.\n",
        "        If the file does not exist, it creates a new file with the provided content.\n",
        "\n",
        "        This function operates on a specific branch named 'test'. If updating, it will commit the changes with a given content tag as the commit message.\n",
        "        In case the file needs to be created, it will also use the content tag as the commit message for the new file.\n",
        "\n",
        "        Parameters:\n",
        "        - repo_name (str): The name of the repository, formatted as 'username/repository'.\n",
        "        - file_path (str): The path to the file within the repository. This should include the file name and its extension.\n",
        "        - file_content (str): The content to be written to the file. This is used both for updating and creating the file.\n",
        "        - content_tag (str): A message associated with the commit used for updating or creating the file.\n",
        "        - branch (str): Github branch for the code\n",
        "\n",
        "        Behavior:\n",
        "        - If the file exists at the specified path, it updates the file with `file_content`, using `content_tag` as the commit message.\n",
        "        - If the file does not exist, it creates a new file at the specified path with `file_content`, also using `content_tag` as the commit message for creation.\n",
        "        - Upon successful update or creation, prints a success message indicating the action taken.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the repository\n",
        "    repo = g.get_repo(repo_name)\n",
        "\n",
        "    try:\n",
        "        # Get the contents of the file if it exists\n",
        "        file = repo.get_contents(file_path, ref=branch)\n",
        "\n",
        "        # Update the file\n",
        "        repo.update_file(file_path, content_tag, file_content, file.sha, branch=branch)\n",
        "        print(f\"File '{file_path}' updated successfully.\")\n",
        "    except:\n",
        "        # If the file doesn't exist, create it\n",
        "        print(f\"{file_path}/{file_content} does not exist\")\n",
        "        repo.create_file(file_path, content_tag, file_content, branch=branch)\n",
        "        print(f\"File '{file_path}' created successfully.\")\n",
        "\n",
        "def create_notebook(response, system_message, instructions, filename):\n",
        "    # Extract summary, code, and explanation from the response JSON\n",
        "    summary = response[\"summary\"]\n",
        "\n",
        "    # Create the notebook content\n",
        "    summary_section = f\"# Databricks notebook source\\n# MAGIC %md\\n# MAGIC # Summary\\n# MAGIC {summary}\\n\"\n",
        "\n",
        "    parts_section = \"\"\n",
        "    for element in response_data['parts']:\n",
        "      parts_section += f\"# COMMAND ----------\\n# MAGIC %md\\n# MAGIC ##{element['sub_header']}\\n# COMMAND ----------\\n# MAGIC %md\\n# MAGIC ## Explanation\\n{element['explanation']}\\n# COMMAND ----------\\n{element['code']}\\n\"\n",
        "\n",
        "    instructions_section = f\"\"\"\n",
        "# COMMAND ----------\n",
        "# MAGIC %md\n",
        "# MAGIC # GenAI Instructions\n",
        "# COMMAND ----------\\n\n",
        "# MAGIC %md\n",
        "# MAGIC * ## System message to AI\n",
        "# MAGIC   * {system_message}\n",
        "\n",
        "# COMMAND ----------\n",
        "# MAGIC %md\n",
        "# MAGIC * ## Instructions to AI (Try edit mode for visualizing table structure)\n",
        "# MAGIC   * {instructions}\n",
        "\"\"\"\n",
        "    notebook_content = summary_section + parts_section + instructions_section\n",
        "    # Write the notebook content to a file\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(notebook_content)\n",
        "\n",
        "    print(f\"Notebook '{filename}' has been created.\")\n",
        "\n",
        "    return notebook_content"
      ],
      "metadata": {
        "id": "U6zhOVp3rhDW"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def convert_str_to_dict(s):\n",
        "    try:\n",
        "        d = ast.literal_eval(s)\n",
        "        if isinstance(d, dict):\n",
        "            return d\n",
        "        else:\n",
        "            raise ValueError(\"Input is not a valid dictionary string\")\n",
        "    except (ValueError, SyntaxError):\n",
        "        raise ValueError(\"Input is not a valid dictionary string\")\n",
        "\n",
        "import string\n",
        "\n",
        "def strip_control_characters_old(s):\n",
        "    # Create a translation table that maps all control characters to None\n",
        "    control_chars = dict.fromkeys(range(0x00, 0x20), ' ')\n",
        "    control_chars.update(dict.fromkeys(range(0x7f, 0xa0), ' '))\n",
        "\n",
        "    # Translate the string using the translation table\n",
        "    cleaned_str = s.translate(dict.fromkeys(control_chars, ' '))\n",
        "\n",
        "    return cleaned_str\n",
        "\n",
        "def strip_control_characters(s):\n",
        "    # Create a translation table that maps all control characters and special characters to a space ' '\n",
        "    control_chars = dict.fromkeys(range(0x00, 0x09), ' ')  # Exclude \\n, \\r, \\f\n",
        "    control_chars.update(dict.fromkeys(range(0x0B, 0x0C), ' '))\n",
        "    control_chars.update(dict.fromkeys(range(0x0E, 0x20), ' '))\n",
        "    control_chars.update(dict.fromkeys(range(0x7f, 0xa0), ' '))\n",
        "    special_chars = dict.fromkeys(map(ord, string.punctuation.replace('\\n', '').replace('\\r', '').replace('\\f', '')), ' ')\n",
        "    control_chars.update(special_chars)\n",
        "\n",
        "    # Translate the string using the translation table\n",
        "    cleaned_str = s.translate(control_chars)"
      ],
      "metadata": {
        "id": "NOGlMd278egD"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "1.   System Message\n",
        "2.   User Message\n",
        "\n"
      ],
      "metadata": {
        "id": "TqsXQmSVUKDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "system_message = \"\"\"\n",
        "You are  Azure Databricks data engineer.\n",
        "    - You will be given tasks and asked to write pyspark code.\n",
        "    - You will use best practices for writing code.\n",
        "    - Your response will be in JSON format with keys summary, number_of_parts, sub_header, code, explanation.\n",
        "    - JSON format must be summary, number_of_parts and parts. Parts must be an array containing code and explanation\n",
        "  \"\"\".strip()\n",
        "\n",
        "user_message_content = read_file_as_string(\"./BookstorePrompt.txt\")\n",
        "print(user_message_content)"
      ],
      "metadata": {
        "id": "CXd8CCYOLCBN",
        "outputId": "613e9d12-741a-4047-fcb8-2d6d6b882bfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please build a pyspark program for Azure Databricks using Medallion framework.\n",
            "- I will give you the table schema. I will provide general instructions and instructions for each step. \n",
            "- The schema for the tables is as follows\n",
            "\n",
            "- customers table schema\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- profile: string (nullable = true)\n",
            " |-- updated: string (nullable = true)\n",
            "\n",
            "- books table schema\n",
            "root\n",
            " |-- book_id: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- author: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " \n",
            "- orders_bronze table schema\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- order_timestamp: long (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- quantity: long (nullable = true)\n",
            " |-- total: integer (nullable = true)\n",
            " |-- books: array (nullable = true)\n",
            " |--|-- element: struct (containsNull = true)\n",
            " |--|--|--- book_id: string (nullable = true)\n",
            " |--|--|--- quantity: integer (nullable = true)\n",
            " |--|--|--- subtotal: long (nullable = true)\n",
            " |-- _rescued_data: string (nullable = true)\n",
            " |-- file_name: string (nullable = true)\n",
            " |-- processed_timestamp: timestamp (nullable = true)\n",
            " \n",
            "\n",
            "- orders_silver schema\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- quantity: long (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- books: array (nullable = true)\n",
            " |-- |-- element: struct (containsNull = true)\n",
            " |--|--|--- book_id: string (nullable = true)\n",
            " |--|--|--- quantity: integer (nullable = true)\n",
            " |--|--|--- subtotal: long (nullable = true)\n",
            " |-- f_name: string (nullable = true)\n",
            " |-- l_name: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " \n",
            "\n",
            "- sales_by_author schema\n",
            "root\n",
            " |-- author: string (nullable = true)\n",
            " |-- Total_Sales_Amount: long (nullable = true)\n",
            " |-- Total_Sales_Quantity: long (nullable = true)\n",
            "\n",
            "General Instructions:\n",
            "\n",
            "1 - Use variables following programming best practices\n",
            "2 - Root directory for the checkpoint folder is dbfs:/mnt/bookstore/checkpoints/\n",
            "3 - Root directory for the schemas folder is dbfs:/mnt/bookstore/schemas/\n",
            "4 - Define necessary sub-folders as required. \n",
            "\n",
            "The instructions are given in three parts\n",
            "\n",
            "Part 1: Ingesting Data into Orders_Bronze Table\n",
            "1 - input folder location for raw data is dbfs:/mnt/bookstore/orders-raw\n",
            "2 - Initialize a Spark session and use Autoloader to ingest data files. The file format is parquet.\n",
            "3 - Append file_name and processed_timestamp columns using input_file_name() and current_timestamp() functions, respectively.\n",
            "4 - Write the stream to target table orders_bronze table. Use Append as output mode. Specify checkpoint location.\n",
            "5 - use .trigger(availableNow=True)\n",
            "6 - Use toTable() function\n",
            "7 - Use Autoloader schema evolution functionality\n",
            "8 - Generated code for part must be placed in code value of the json object\n",
            "9 - Explanation for the part must be placed in code value of the json object \n",
            "\n",
            "\n",
            "Part 2: Writing Orders_Silver Table from Orders_Bronze and Customers\n",
            "1 - Table names and locations are stored in variables\n",
            "1 - Read from orders_bronze table as a stream and create a temporary streaming view named orders_bronze_streaming_view.\n",
            "2 - Load the customers table\n",
            "3 - Perform a SQL join between orders_bronze_streaming_view and customers table on customer_id. Selecting order details. Extract first_name and last_name from profile column. Profile column is a json object.\n",
            "4 - select only rows that have quantity greater than 0\n",
            "5 - Write the joined data to the orders_silver table using writeStream with append mode. Specify checkpoint location.\n",
            "6 - use .trigger(availableNow=True)\n",
            "7 - Use toTable() function\n",
            "8 - Generated code for part must be placed in code value of the json object\n",
            "9 - Explanation for the part must be placed in code value of the json object \n",
            "\n",
            "Part 3: Populating Sales_by_Author Table from Orders_Silver\n",
            "1 - Table names and locations are stored in variables\n",
            "2 - Read from the orders_silver table as a stream, explode the books array to flatten the book details, and select necessary columns for processing.\n",
            "3 - Assume the books table exists and contains book_id and author, perform a join to enrich book items with author.\n",
            "4 - Group by author and aggregate to calculate Total_Sales_Amount and Total_Sales_Quantity.\n",
            "5 - Write the aggregated data to the sales_by_author table using writeStream\n",
            "6 - use .trigger(availableNow=True) \n",
            "7 - Use toTable() function\n",
            "8 - Generated code for part must be placed in code value of the json object\n",
            "9 - Explanation for the part must be placed in code value of the json object \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(system_message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pvoBFnZNy6l",
        "outputId": "b260c5e6-fd9e-4073-9f61-1b913a71c6a3"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are  Azure Databricks data engineer.\n",
            "    - You will be given tasks and asked to write pyspark code.\n",
            "    - You will use best practices for writing code.\n",
            "    - Your response will be in JSON format with keys summary, number_of_parts, sub_header, code, explanation.\n",
            "    - JSON format must be summary, number_of_parts and parts. Parts must be an array containing code and explanation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make the call to LLMs"
      ],
      "metadata": {
        "id": "_MDSbrT9UpOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the message with variables\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    temperature = 0,\n",
        "    response_format = {\"type\" : \"json_object\"},\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message_content}]\n",
        ")\n",
        "\n",
        "# Assuming you have a client setup for interaction. Ensure to configure your OpenAI client appropriately.\n",
        "\n"
      ],
      "metadata": {
        "id": "DAKT9wxMrhGK"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nResponse id: {response.id}\\nCreated: {response.created}\\nModel: {response.model}\\nCompletion Tokens: {response.usage.completion_tokens}\\nPrompt Tokens: {response.usage.prompt_tokens}\\nTotal tokens: {response.usage.total_tokens}\")\n",
        "#print(response.created)\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYXOwUpo-MqT",
        "outputId": "a6b88113-70bb-4a90-adc8-870255271c9b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response id: chatcmpl-93YlkUL670ob8KCR4GFuWRCcCsY4C\n",
            "Created: 1710635664\n",
            "Model: gpt-4-1106-preview\n",
            "Completion Tokens: 1348\n",
            "Prompt Tokens: 1153\n",
            "Total tokens: 2501\n",
            "{\n",
            "    \"summary\": \"Medallion architecture implementation in Azure Databricks using PySpark\",\n",
            "    \"number_of_parts\": 3,\n",
            "    \"parts\": [\n",
            "        {\n",
            "            \"sub_header\": \"Part 1: Ingesting Data into Orders_Bronze Table\",\n",
            "            \"code\": \"from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import input_file_name, current_timestamp\\n\\n# Initialize Spark session\\nspark = SparkSession.builder.appName('OrdersBronzeIngestion').getOrCreate()\\n\\n# Define variables\\ninput_folder = 'dbfs:/mnt/bookstore/orders-raw'\\ncheckpoint_location = 'dbfs:/mnt/bookstore/checkpoints/orders_bronze'\\ntarget_table = 'orders_bronze'\\n\\n# Ingest data using AutoLoader\\nbronze_df = (spark.readStream.format('cloudFiles')\\n    .option('cloudFiles.format', 'parquet')\\n    .option('cloudFiles.schemaLocation', checkpoint_location)\\n    .option('cloudFiles.schemaEvolutionMode', 'addNewColumns')\\n    .load(input_folder)\\n    .withColumn('file_name', input_file_name())\\n    .withColumn('processed_timestamp', current_timestamp())\\n)\\n\\n# Write stream to target table\\n(bronze_df.writeStream.format('delta')\\n    .outputMode('append')\\n    .option('checkpointLocation', checkpoint_location)\\n    .trigger(availableNow=True)\\n    .toTable(target_table)\\n)\",\n",
            "            \"explanation\": \"This code initializes a Spark session and sets up the AutoLoader to ingest data from the specified input folder. The data is read in parquet format, and schema evolution is enabled to handle any new columns in the data. The input_file_name and current_timestamp functions are used to append the file_name and processed_timestamp columns. The data is then written to the orders_bronze table in append mode, with a checkpoint location specified for fault tolerance. The trigger option 'availableNow' is used to process the available files immediately.\"\n",
            "        },\n",
            "        {\n",
            "            \"sub_header\": \"Part 2: Writing Orders_Silver Table from Orders_Bronze and Customers\",\n",
            "            \"code\": \"from pyspark.sql.functions import from_json, col\\n\\n# Define table names and locations\\norders_bronze_table = 'orders_bronze'\\ncustomers_table = 'customers'\\norders_silver_table = 'orders_silver'\\ncheckpoint_location_silver = 'dbfs:/mnt/bookstore/checkpoints/orders_silver'\\n\\n# Read from orders_bronze table as a stream\\norders_bronze_df = spark.readStream.table(orders_bronze_table)\\norders_bronze_df.createOrReplaceTempView('orders_bronze_streaming_view')\\n\\n# Load customers table\\ncustomers_df = spark.read.table(customers_table)\\n\\n# Perform SQL join and select\\njoined_df = spark.sql(\\\"\\\"\\\"\\n    SELECT ob.order_id, ob.quantity, ob.customer_id, ob.books,\\n           get_json_object(c.profile, '$.first_name') AS f_name,\\n           get_json_object(c.profile, '$.last_name') AS l_name,\\n           ob.order_date\\n    FROM orders_bronze_streaming_view ob\\n    JOIN customers c ON ob.customer_id = c.customer_id\\n    WHERE ob.quantity > 0\\n\\\"\\\"\\\")\\n\\n# Write the joined data to the orders_silver table\\n(joined_df.writeStream.format('delta')\\n    .outputMode('append')\\n    .option('checkpointLocation', checkpoint_location_silver)\\n    .trigger(availableNow=True)\\n    .toTable(orders_silver_table)\\n)\",\n",
            "            \"explanation\": \"This code reads the orders_bronze table as a stream and creates a temporary streaming view. It then loads the customers table and performs a SQL join with the streaming view on the customer_id column. The join extracts first_name and last_name from the profile JSON column. It filters out rows with a quantity of 0 or less. The resulting DataFrame is written to the orders_silver table in append mode with a checkpoint location specified. The trigger option 'availableNow' is used to process the available data immediately.\"\n",
            "        },\n",
            "        {\n",
            "            \"sub_header\": \"Part 3: Populating Sales_by_Author Table from Orders_Silver\",\n",
            "            \"code\": \"from pyspark.sql.functions import explode, sum as _sum, col\\n\\n# Define table names and locations\\norders_silver_table = 'orders_silver'\\nbooks_table = 'books'\\nsales_by_author_table = 'sales_by_author'\\ncheckpoint_location_sales_by_author = 'dbfs:/mnt/bookstore/checkpoints/sales_by_author'\\n\\n# Read from the orders_silver table as a stream\\norders_silver_df = spark.readStream.table(orders_silver_table)\\n\\n# Explode books array and select necessary columns\\nbooks_exploded_df = orders_silver_df.selectExpr('explode(books) as book', '*')\\n\\n# Join with books table to enrich with author\\nsales_df = books_exploded_df.join(spark.read.table(books_table), books_exploded_df.book.book_id == col('book_id'))\\n\\n# Group by author and aggregate\\nsales_by_author_df = sales_df.groupBy('author')\\n    .agg(_sum('book.subtotal').alias('Total_Sales_Amount'),\\n         _sum('book.quantity').alias('Total_Sales_Quantity'))\\n\\n# Write the aggregated data to the sales_by_author table\\n(sales_by_author_df.writeStream.format('delta')\\n    .outputMode('complete')\\n    .option('checkpointLocation', checkpoint_location_sales_by_author)\\n    .trigger(availableNow=True)\\n    .toTable(sales_by_author_table)\\n)\",\n",
            "            \"explanation\": \"This code reads the orders_silver table as a stream and explodes the books array to flatten the book details. It then joins the resulting DataFrame with the books table to enrich the book items with the author information. The data is grouped by author, and the total sales amount and quantity are aggregated. The aggregated data is written to the sales_by_author table using the complete output mode, which is suitable for aggregations. A checkpoint location is specified for fault tolerance, and the trigger option 'availableNow' is used to process the available data immediately.\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate response from LLM"
      ],
      "metadata": {
        "id": "MOTbodyRUt55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pMjSEaIcLrNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_data = json.loads(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "UeTQe5VzZ56L"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_data['summary']\n",
        "response_data['parts'][0]"
      ],
      "metadata": {
        "id": "1jm3dTpVaG07",
        "outputId": "4d9438a4-a6b1-4632-e2ed-17753d91c128",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sub_header': 'Part 1: Ingesting Data into Orders_Bronze Table',\n",
              " 'code': \"from pyspark.sql.functions import input_file_name, current_timestamp\\nfrom pyspark.sql import SparkSession\\n\\n# Initialize Spark session\\nspark = SparkSession.builder.appName('OrdersBronzeIngestion').getOrCreate()\\n\\n# Define variables\\ninput_folder = 'dbfs:/mnt/bookstore/orders-raw'\\ncheckpoint_location = 'dbfs:/mnt/bookstore/checkpoints/orders_bronze'\\ntarget_table = 'orders_bronze'\\n\\n# Ingest data using AutoLoader\\norders_bronze_df = (spark.readStream.format('cloudFiles')\\n    .option('cloudFiles.format', 'parquet')\\n    .option('cloudFiles.schemaLocation', checkpoint_location)\\n    .option('cloudFiles.schemaEvolutionMode', 'addNewColumns')\\n    .load(input_folder)\\n    .withColumn('file_name', input_file_name())\\n    .withColumn('processed_timestamp', current_timestamp())\\n)\\n\\n# Write to table\\n(orders_bronze_df.writeStream.format('delta')\\n    .outputMode('append')\\n    .option('checkpointLocation', checkpoint_location)\\n    .trigger(availableNow=True)\\n    .toTable(target_table)\\n)\",\n",
              " 'explanation': \"This code initializes a Spark session and sets up the ingestion of data from the specified input folder using the AutoLoader feature. The data is read in parquet format, and schema evolution is enabled to add new columns as they appear in the source data. The file_name and processed_timestamp columns are appended to the DataFrame. The data is then written to the orders_bronze table in append mode, with a checkpoint location specified for fault tolerance. The trigger option 'availableNow' is used to process the available files immediately.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_contents = create_notebook(response_data, system_message, user_message_content, \"orders_bronze_notebook-t2.py\")"
      ],
      "metadata": {
        "id": "F3ZTQpLg3RiI",
        "outputId": "601d0fd0-c253-404d-cbd1-24eb26244144",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook 'orders_bronze_notebook-t2.py' has been created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(file_contents)"
      ],
      "metadata": {
        "id": "wjTxx8GwZV6k",
        "outputId": "392ad006-1fa6-4956-8fa3-0fd2f113edd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Databricks notebook source\n",
            "# MAGIC %md\n",
            "# MAGIC # Summary\n",
            "# MAGIC Medallion architecture implementation in Azure Databricks using PySpark\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC ##Part 1: Ingesting Data into Orders_Bronze Table\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC ## Explanation\n",
            "This code initializes a Spark session and sets up the AutoLoader to ingest data from the specified input folder. The data is read in parquet format, and schema evolution is enabled to handle any new columns in the data. The input_file_name and current_timestamp functions are used to append the file_name and processed_timestamp columns. The data is then written to the orders_bronze table in append mode, with a checkpoint location specified for fault tolerance. The trigger option 'availableNow' is used to process the available files immediately.\n",
            "# COMMAND ----------\n",
            "from pyspark.sql import SparkSession\n",
            "from pyspark.sql.functions import input_file_name, current_timestamp\n",
            "\n",
            "# Initialize Spark session\n",
            "spark = SparkSession.builder.appName('OrdersBronzeIngestion').getOrCreate()\n",
            "\n",
            "# Define variables\n",
            "input_folder = 'dbfs:/mnt/bookstore/orders-raw'\n",
            "checkpoint_location = 'dbfs:/mnt/bookstore/checkpoints/orders_bronze'\n",
            "target_table = 'orders_bronze'\n",
            "\n",
            "# Ingest data using AutoLoader\n",
            "bronze_df = (spark.readStream.format('cloudFiles')\n",
            "    .option('cloudFiles.format', 'parquet')\n",
            "    .option('cloudFiles.schemaLocation', checkpoint_location)\n",
            "    .option('cloudFiles.schemaEvolutionMode', 'addNewColumns')\n",
            "    .load(input_folder)\n",
            "    .withColumn('file_name', input_file_name())\n",
            "    .withColumn('processed_timestamp', current_timestamp())\n",
            ")\n",
            "\n",
            "# Write stream to target table\n",
            "(bronze_df.writeStream.format('delta')\n",
            "    .outputMode('append')\n",
            "    .option('checkpointLocation', checkpoint_location)\n",
            "    .trigger(availableNow=True)\n",
            "    .toTable(target_table)\n",
            ")\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC ##Part 2: Writing Orders_Silver Table from Orders_Bronze and Customers\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC ## Explanation\n",
            "This code reads the orders_bronze table as a stream and creates a temporary streaming view. It then loads the customers table and performs a SQL join with the streaming view on the customer_id column. The join extracts first_name and last_name from the profile JSON column. It filters out rows with a quantity of 0 or less. The resulting DataFrame is written to the orders_silver table in append mode with a checkpoint location specified. The trigger option 'availableNow' is used to process the available data immediately.\n",
            "# COMMAND ----------\n",
            "from pyspark.sql.functions import from_json, col\n",
            "\n",
            "# Define table names and locations\n",
            "orders_bronze_table = 'orders_bronze'\n",
            "customers_table = 'customers'\n",
            "orders_silver_table = 'orders_silver'\n",
            "checkpoint_location_silver = 'dbfs:/mnt/bookstore/checkpoints/orders_silver'\n",
            "\n",
            "# Read from orders_bronze table as a stream\n",
            "orders_bronze_df = spark.readStream.table(orders_bronze_table)\n",
            "orders_bronze_df.createOrReplaceTempView('orders_bronze_streaming_view')\n",
            "\n",
            "# Load customers table\n",
            "customers_df = spark.read.table(customers_table)\n",
            "\n",
            "# Perform SQL join and select\n",
            "joined_df = spark.sql(\"\"\"\n",
            "    SELECT ob.order_id, ob.quantity, ob.customer_id, ob.books,\n",
            "           get_json_object(c.profile, '$.first_name') AS f_name,\n",
            "           get_json_object(c.profile, '$.last_name') AS l_name,\n",
            "           ob.order_date\n",
            "    FROM orders_bronze_streaming_view ob\n",
            "    JOIN customers c ON ob.customer_id = c.customer_id\n",
            "    WHERE ob.quantity > 0\n",
            "\"\"\")\n",
            "\n",
            "# Write the joined data to the orders_silver table\n",
            "(joined_df.writeStream.format('delta')\n",
            "    .outputMode('append')\n",
            "    .option('checkpointLocation', checkpoint_location_silver)\n",
            "    .trigger(availableNow=True)\n",
            "    .toTable(orders_silver_table)\n",
            ")\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC ##Part 3: Populating Sales_by_Author Table from Orders_Silver\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC ## Explanation\n",
            "This code reads the orders_silver table as a stream and explodes the books array to flatten the book details. It then joins the resulting DataFrame with the books table to enrich the book items with the author information. The data is grouped by author, and the total sales amount and quantity are aggregated. The aggregated data is written to the sales_by_author table using the complete output mode, which is suitable for aggregations. A checkpoint location is specified for fault tolerance, and the trigger option 'availableNow' is used to process the available data immediately.\n",
            "# COMMAND ----------\n",
            "from pyspark.sql.functions import explode, sum as _sum, col\n",
            "\n",
            "# Define table names and locations\n",
            "orders_silver_table = 'orders_silver'\n",
            "books_table = 'books'\n",
            "sales_by_author_table = 'sales_by_author'\n",
            "checkpoint_location_sales_by_author = 'dbfs:/mnt/bookstore/checkpoints/sales_by_author'\n",
            "\n",
            "# Read from the orders_silver table as a stream\n",
            "orders_silver_df = spark.readStream.table(orders_silver_table)\n",
            "\n",
            "# Explode books array and select necessary columns\n",
            "books_exploded_df = orders_silver_df.selectExpr('explode(books) as book', '*')\n",
            "\n",
            "# Join with books table to enrich with author\n",
            "sales_df = books_exploded_df.join(spark.read.table(books_table), books_exploded_df.book.book_id == col('book_id'))\n",
            "\n",
            "# Group by author and aggregate\n",
            "sales_by_author_df = sales_df.groupBy('author')\n",
            "    .agg(_sum('book.subtotal').alias('Total_Sales_Amount'),\n",
            "         _sum('book.quantity').alias('Total_Sales_Quantity'))\n",
            "\n",
            "# Write the aggregated data to the sales_by_author table\n",
            "(sales_by_author_df.writeStream.format('delta')\n",
            "    .outputMode('complete')\n",
            "    .option('checkpointLocation', checkpoint_location_sales_by_author)\n",
            "    .trigger(availableNow=True)\n",
            "    .toTable(sales_by_author_table)\n",
            ")\n",
            "\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC # GenAI Instructions\n",
            "# COMMAND ----------\n",
            "\n",
            "# MAGIC %md\n",
            "# MAGIC * ## System message to AI\n",
            "# MAGIC   * You are  Azure Databricks data engineer.\n",
            "    - You will be given tasks and asked to write pyspark code.\n",
            "    - You will use best practices for writing code.\n",
            "    - Your response will be in JSON format with keys summary, number_of_parts, sub_header, code, explanation.\n",
            "    - JSON format must be summary, number_of_parts and parts. Parts must be an array containing code and explanation\n",
            "\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC * ## Instructions to AI (Try edit mode for visualizing table structure)\n",
            "# MAGIC   * Please build a pyspark program for Azure Databricks using Medallion framework.\n",
            "- I will give you the table schema. I will provide general instructions and instructions for each step. \n",
            "- The schema for the tables is as follows\n",
            "\n",
            "- customers table schema\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- profile: string (nullable = true)\n",
            " |-- updated: string (nullable = true)\n",
            "\n",
            "- books table schema\n",
            "root\n",
            " |-- book_id: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- author: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " \n",
            "- orders_bronze table schema\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- order_timestamp: long (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- quantity: long (nullable = true)\n",
            " |-- total: integer (nullable = true)\n",
            " |-- books: array (nullable = true)\n",
            " |--|-- element: struct (containsNull = true)\n",
            " |--|--|--- book_id: string (nullable = true)\n",
            " |--|--|--- quantity: integer (nullable = true)\n",
            " |--|--|--- subtotal: long (nullable = true)\n",
            " |-- _rescued_data: string (nullable = true)\n",
            " |-- file_name: string (nullable = true)\n",
            " |-- processed_timestamp: timestamp (nullable = true)\n",
            " \n",
            "\n",
            "- orders_silver schema\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- quantity: long (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- books: array (nullable = true)\n",
            " |-- |-- element: struct (containsNull = true)\n",
            " |--|--|--- book_id: string (nullable = true)\n",
            " |--|--|--- quantity: integer (nullable = true)\n",
            " |--|--|--- subtotal: long (nullable = true)\n",
            " |-- f_name: string (nullable = true)\n",
            " |-- l_name: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " \n",
            "\n",
            "- sales_by_author schema\n",
            "root\n",
            " |-- author: string (nullable = true)\n",
            " |-- Total_Sales_Amount: long (nullable = true)\n",
            " |-- Total_Sales_Quantity: long (nullable = true)\n",
            "\n",
            "General Instructions:\n",
            "\n",
            "1 - Use variables following programming best practices\n",
            "2 - Root directory for the checkpoint folder is dbfs:/mnt/bookstore/checkpoints/\n",
            "3 - Root directory for the schemas folder is dbfs:/mnt/bookstore/schemas/\n",
            "4 - Define necessary sub-folders as required. \n",
            "\n",
            "The instructions are given in three parts\n",
            "\n",
            "Part 1: Ingesting Data into Orders_Bronze Table\n",
            "1 - input folder location for raw data is dbfs:/mnt/bookstore/orders-raw\n",
            "2 - Initialize a Spark session and use Autoloader to ingest data files. The file format is parquet.\n",
            "3 - Append file_name and processed_timestamp columns using input_file_name() and current_timestamp() functions, respectively.\n",
            "4 - Write the stream to target table orders_bronze table. Use Append as output mode. Specify checkpoint location.\n",
            "5 - use .trigger(availableNow=True)\n",
            "6 - Use toTable() function\n",
            "7 - Use Autoloader schema evolution functionality\n",
            "8 - Generated code for part must be placed in code value of the json object\n",
            "9 - Explanation for the part must be placed in code value of the json object \n",
            "\n",
            "\n",
            "Part 2: Writing Orders_Silver Table from Orders_Bronze and Customers\n",
            "1 - Table names and locations are stored in variables\n",
            "1 - Read from orders_bronze table as a stream and create a temporary streaming view named orders_bronze_streaming_view.\n",
            "2 - Load the customers table\n",
            "3 - Perform a SQL join between orders_bronze_streaming_view and customers table on customer_id. Selecting order details. Extract first_name and last_name from profile column. Profile column is a json object.\n",
            "4 - select only rows that have quantity greater than 0\n",
            "5 - Write the joined data to the orders_silver table using writeStream with append mode. Specify checkpoint location.\n",
            "6 - use .trigger(availableNow=True)\n",
            "7 - Use toTable() function\n",
            "8 - Generated code for part must be placed in code value of the json object\n",
            "9 - Explanation for the part must be placed in code value of the json object \n",
            "\n",
            "Part 3: Populating Sales_by_Author Table from Orders_Silver\n",
            "1 - Table names and locations are stored in variables\n",
            "2 - Read from the orders_silver table as a stream, explode the books array to flatten the book details, and select necessary columns for processing.\n",
            "3 - Assume the books table exists and contains book_id and author, perform a join to enrich book items with author.\n",
            "4 - Group by author and aggregate to calculate Total_Sales_Amount and Total_Sales_Quantity.\n",
            "5 - Write the aggregated data to the sales_by_author table using writeStream\n",
            "6 - use .trigger(availableNow=True) \n",
            "7 - Use toTable() function\n",
            "8 - Generated code for part must be placed in code value of the json object\n",
            "9 - Explanation for the part must be placed in code value of the json object \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check into Github\n",
        "*   repository : \"cooolbabu/GoogleGemini101\"\n",
        "*   filename : \"AzureDatabricks/filename\" - specify actual filename\n",
        "*   filecontent: Contents of the file to check in\n",
        "*   tag_name: give a comment. It will show in Github\n",
        "* branch: branch name to check into. Ensure that branch already exists\n",
        "          Future TODO: if branch doesn't exist (notify, ask, process)\n",
        "\n"
      ],
      "metadata": {
        "id": "zn-veyM-U19x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "check_in_file(repo_name=\"cooolbabu/GoogleGemini101\",\n",
        "              file_path=\"AzureDatabricks/ConfigureDB/orders_streams_p3.py\",\n",
        "              file_content=file_contents,\n",
        "              content_tag='Orders streaming p3',\n",
        "              branch=\"pyspark-genai-t3\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ELMDUfXRBi7V",
        "outputId": "910f3a0f-59f6-4a13-9b2c-baa19a579636",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AzureDatabricks/ConfigureDB/orders_streams_p3.py/# Databricks notebook source\n",
            "# MAGIC %md\n",
            "# MAGIC # Summary\n",
            "# MAGIC Medallion architecture implementation in Azure Databricks using PySpark\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC ##Part 1: Ingesting Data into Orders_Bronze Table\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC ## Explanation\n",
            "This code initializes a Spark session and sets up the AutoLoader to ingest data from the specified input folder. The data is read in parquet format, and schema evolution is enabled to handle any new columns in the data. The input_file_name and current_timestamp functions are used to append the file_name and processed_timestamp columns. The data is then written to the orders_bronze table in append mode, with a checkpoint location specified for fault tolerance. The trigger option 'availableNow' is used to process the available files immediately.\n",
            "# COMMAND ----------\n",
            "from pyspark.sql import SparkSession\n",
            "from pyspark.sql.functions import input_file_name, current_timestamp\n",
            "\n",
            "# Initialize Spark session\n",
            "spark = SparkSession.builder.appName('OrdersBronzeIngestion').getOrCreate()\n",
            "\n",
            "# Define variables\n",
            "input_folder = 'dbfs:/mnt/bookstore/orders-raw'\n",
            "checkpoint_location = 'dbfs:/mnt/bookstore/checkpoints/orders_bronze'\n",
            "target_table = 'orders_bronze'\n",
            "\n",
            "# Ingest data using AutoLoader\n",
            "bronze_df = (spark.readStream.format('cloudFiles')\n",
            "    .option('cloudFiles.format', 'parquet')\n",
            "    .option('cloudFiles.schemaLocation', checkpoint_location)\n",
            "    .option('cloudFiles.schemaEvolutionMode', 'addNewColumns')\n",
            "    .load(input_folder)\n",
            "    .withColumn('file_name', input_file_name())\n",
            "    .withColumn('processed_timestamp', current_timestamp())\n",
            ")\n",
            "\n",
            "# Write stream to target table\n",
            "(bronze_df.writeStream.format('delta')\n",
            "    .outputMode('append')\n",
            "    .option('checkpointLocation', checkpoint_location)\n",
            "    .trigger(availableNow=True)\n",
            "    .toTable(target_table)\n",
            ")\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC ##Part 2: Writing Orders_Silver Table from Orders_Bronze and Customers\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC ## Explanation\n",
            "This code reads the orders_bronze table as a stream and creates a temporary streaming view. It then loads the customers table and performs a SQL join with the streaming view on the customer_id column. The join extracts first_name and last_name from the profile JSON column. It filters out rows with a quantity of 0 or less. The resulting DataFrame is written to the orders_silver table in append mode with a checkpoint location specified. The trigger option 'availableNow' is used to process the available data immediately.\n",
            "# COMMAND ----------\n",
            "from pyspark.sql.functions import from_json, col\n",
            "\n",
            "# Define table names and locations\n",
            "orders_bronze_table = 'orders_bronze'\n",
            "customers_table = 'customers'\n",
            "orders_silver_table = 'orders_silver'\n",
            "checkpoint_location_silver = 'dbfs:/mnt/bookstore/checkpoints/orders_silver'\n",
            "\n",
            "# Read from orders_bronze table as a stream\n",
            "orders_bronze_df = spark.readStream.table(orders_bronze_table)\n",
            "orders_bronze_df.createOrReplaceTempView('orders_bronze_streaming_view')\n",
            "\n",
            "# Load customers table\n",
            "customers_df = spark.read.table(customers_table)\n",
            "\n",
            "# Perform SQL join and select\n",
            "joined_df = spark.sql(\"\"\"\n",
            "    SELECT ob.order_id, ob.quantity, ob.customer_id, ob.books,\n",
            "           get_json_object(c.profile, '$.first_name') AS f_name,\n",
            "           get_json_object(c.profile, '$.last_name') AS l_name,\n",
            "           ob.order_date\n",
            "    FROM orders_bronze_streaming_view ob\n",
            "    JOIN customers c ON ob.customer_id = c.customer_id\n",
            "    WHERE ob.quantity > 0\n",
            "\"\"\")\n",
            "\n",
            "# Write the joined data to the orders_silver table\n",
            "(joined_df.writeStream.format('delta')\n",
            "    .outputMode('append')\n",
            "    .option('checkpointLocation', checkpoint_location_silver)\n",
            "    .trigger(availableNow=True)\n",
            "    .toTable(orders_silver_table)\n",
            ")\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC ##Part 3: Populating Sales_by_Author Table from Orders_Silver\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC ## Explanation\n",
            "This code reads the orders_silver table as a stream and explodes the books array to flatten the book details. It then joins the resulting DataFrame with the books table to enrich the book items with the author information. The data is grouped by author, and the total sales amount and quantity are aggregated. The aggregated data is written to the sales_by_author table using the complete output mode, which is suitable for aggregations. A checkpoint location is specified for fault tolerance, and the trigger option 'availableNow' is used to process the available data immediately.\n",
            "# COMMAND ----------\n",
            "from pyspark.sql.functions import explode, sum as _sum, col\n",
            "\n",
            "# Define table names and locations\n",
            "orders_silver_table = 'orders_silver'\n",
            "books_table = 'books'\n",
            "sales_by_author_table = 'sales_by_author'\n",
            "checkpoint_location_sales_by_author = 'dbfs:/mnt/bookstore/checkpoints/sales_by_author'\n",
            "\n",
            "# Read from the orders_silver table as a stream\n",
            "orders_silver_df = spark.readStream.table(orders_silver_table)\n",
            "\n",
            "# Explode books array and select necessary columns\n",
            "books_exploded_df = orders_silver_df.selectExpr('explode(books) as book', '*')\n",
            "\n",
            "# Join with books table to enrich with author\n",
            "sales_df = books_exploded_df.join(spark.read.table(books_table), books_exploded_df.book.book_id == col('book_id'))\n",
            "\n",
            "# Group by author and aggregate\n",
            "sales_by_author_df = sales_df.groupBy('author')\n",
            "    .agg(_sum('book.subtotal').alias('Total_Sales_Amount'),\n",
            "         _sum('book.quantity').alias('Total_Sales_Quantity'))\n",
            "\n",
            "# Write the aggregated data to the sales_by_author table\n",
            "(sales_by_author_df.writeStream.format('delta')\n",
            "    .outputMode('complete')\n",
            "    .option('checkpointLocation', checkpoint_location_sales_by_author)\n",
            "    .trigger(availableNow=True)\n",
            "    .toTable(sales_by_author_table)\n",
            ")\n",
            "\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC # GenAI Instructions\n",
            "# COMMAND ----------\n",
            "\n",
            "# MAGIC %md\n",
            "# MAGIC * ## System message to AI\n",
            "# MAGIC   * You are  Azure Databricks data engineer.\n",
            "    - You will be given tasks and asked to write pyspark code.\n",
            "    - You will use best practices for writing code.\n",
            "    - Your response will be in JSON format with keys summary, number_of_parts, sub_header, code, explanation.\n",
            "    - JSON format must be summary, number_of_parts and parts. Parts must be an array containing code and explanation\n",
            "\n",
            "# COMMAND ----------\n",
            "# MAGIC %md\n",
            "# MAGIC * ## Instructions to AI (Try edit mode for visualizing table structure)\n",
            "# MAGIC   * Please build a pyspark program for Azure Databricks using Medallion framework.\n",
            "- I will give you the table schema. I will provide general instructions and instructions for each step. \n",
            "- The schema for the tables is as follows\n",
            "\n",
            "- customers table schema\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- profile: string (nullable = true)\n",
            " |-- updated: string (nullable = true)\n",
            "\n",
            "- books table schema\n",
            "root\n",
            " |-- book_id: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- author: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " \n",
            "- orders_bronze table schema\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- order_timestamp: long (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- quantity: long (nullable = true)\n",
            " |-- total: integer (nullable = true)\n",
            " |-- books: array (nullable = true)\n",
            " |--|-- element: struct (containsNull = true)\n",
            " |--|--|--- book_id: string (nullable = true)\n",
            " |--|--|--- quantity: integer (nullable = true)\n",
            " |--|--|--- subtotal: long (nullable = true)\n",
            " |-- _rescued_data: string (nullable = true)\n",
            " |-- file_name: string (nullable = true)\n",
            " |-- processed_timestamp: timestamp (nullable = true)\n",
            " \n",
            "\n",
            "- orders_silver schema\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- quantity: long (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- books: array (nullable = true)\n",
            " |-- |-- element: struct (containsNull = true)\n",
            " |--|--|--- book_id: string (nullable = true)\n",
            " |--|--|--- quantity: integer (nullable = true)\n",
            " |--|--|--- subtotal: long (nullable = true)\n",
            " |-- f_name: string (nullable = true)\n",
            " |-- l_name: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " \n",
            "\n",
            "- sales_by_author schema\n",
            "root\n",
            " |-- author: string (nullable = true)\n",
            " |-- Total_Sales_Amount: long (nullable = true)\n",
            " |-- Total_Sales_Quantity: long (nullable = true)\n",
            "\n",
            "General Instructions:\n",
            "\n",
            "1 - Use variables following programming best practices\n",
            "2 - Root directory for the checkpoint folder is dbfs:/mnt/bookstore/checkpoints/\n",
            "3 - Root directory for the schemas folder is dbfs:/mnt/bookstore/schemas/\n",
            "4 - Define necessary sub-folders as required. \n",
            "\n",
            "The instructions are given in three parts\n",
            "\n",
            "Part 1: Ingesting Data into Orders_Bronze Table\n",
            "1 - input folder location for raw data is dbfs:/mnt/bookstore/orders-raw\n",
            "2 - Initialize a Spark session and use Autoloader to ingest data files. The file format is parquet.\n",
            "3 - Append file_name and processed_timestamp columns using input_file_name() and current_timestamp() functions, respectively.\n",
            "4 - Write the stream to target table orders_bronze table. Use Append as output mode. Specify checkpoint location.\n",
            "5 - use .trigger(availableNow=True)\n",
            "6 - Use toTable() function\n",
            "7 - Use Autoloader schema evolution functionality\n",
            "8 - Generated code for part must be placed in code value of the json object\n",
            "9 - Explanation for the part must be placed in code value of the json object \n",
            "\n",
            "\n",
            "Part 2: Writing Orders_Silver Table from Orders_Bronze and Customers\n",
            "1 - Table names and locations are stored in variables\n",
            "1 - Read from orders_bronze table as a stream and create a temporary streaming view named orders_bronze_streaming_view.\n",
            "2 - Load the customers table\n",
            "3 - Perform a SQL join between orders_bronze_streaming_view and customers table on customer_id. Selecting order details. Extract first_name and last_name from profile column. Profile column is a json object.\n",
            "4 - select only rows that have quantity greater than 0\n",
            "5 - Write the joined data to the orders_silver table using writeStream with append mode. Specify checkpoint location.\n",
            "6 - use .trigger(availableNow=True)\n",
            "7 - Use toTable() function\n",
            "8 - Generated code for part must be placed in code value of the json object\n",
            "9 - Explanation for the part must be placed in code value of the json object \n",
            "\n",
            "Part 3: Populating Sales_by_Author Table from Orders_Silver\n",
            "1 - Table names and locations are stored in variables\n",
            "2 - Read from the orders_silver table as a stream, explode the books array to flatten the book details, and select necessary columns for processing.\n",
            "3 - Assume the books table exists and contains book_id and author, perform a join to enrich book items with author.\n",
            "4 - Group by author and aggregate to calculate Total_Sales_Amount and Total_Sales_Quantity.\n",
            "5 - Write the aggregated data to the sales_by_author table using writeStream\n",
            "6 - use .trigger(availableNow=True) \n",
            "7 - Use toTable() function\n",
            "8 - Generated code for part must be placed in code value of the json object\n",
            "9 - Explanation for the part must be placed in code value of the json object \n",
            " does not exist\n",
            "File 'AzureDatabricks/ConfigureDB/orders_streams_p3.py' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content_dict = json.loads(response.choices[0].message.content)\n",
        "\n",
        "code_snippets = {}\n",
        "explanations = {}\n",
        "for part, details in content_dict['code'].items():\n",
        "  for task, task_details in details.items():\n",
        "      code_snippets[part] = task_details['code']\n",
        "      explanations[part] = task_details['explanation']\n",
        "\n",
        "print(content_dict[\"summary\"])\n",
        "print(code_snippets)\n",
        "print(explanations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13OH-Av_7lHA",
        "outputId": "69e75893-ba5d-4b99-992c-6c3cd15cf490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created a Pyspark program using Medallion framework for Azure Databricks following the instructions provided.\n",
            "{'part1': \"spark = SparkSession.builder.appName('OrderBronzeIngest').getOrCreate()\\n\\norders_bronze_df = spark.readStream.format('cloudFiles')\\n    .option('cloudFiles.format', 'parquet')\\n    .option('cloudFiles.includeExistingFiles', 'true')\\n    .option('cloudFiles.useNotifications', 'true')\\n    .option('cloudFiles.url', 'dbfs:/mnt/bookstore/orders-raw')\\n    .option('cloudFiles.format', 'parquet')\\n    .load()\\n\\norders_bronze_df = orders_bronze_df.withColumn('file_name', input_file_name())\\n    .withColumn('processed_timestamp', current_timestamp())\\n\\ncheckpoint_location = 'dbfs:/mnt/bookstore/checkpoints/orders_bronze'\\n\\nquery = orders_bronze_df.writeStream.format('delta')\\n    .outputMode('append')\\n    .option('checkpointLocation', checkpoint_location)\\n    .trigger(availableNow=True)\\n    .table('orders_bronze')\", 'part2': \"customers_table = 'customers'\\norders_bronze_table = 'orders_bronze'\\norders_silver_table = 'orders_silver'\\n\\norders_bronze_streaming_view = spark.readStream.format('delta').table(orders_bronze_table)\\n\\njoined_data = orders_bronze_streaming_view.join(F.broadcast(spark.table(customers_table)),\\n    orders_bronze_streaming_view.customer_id == spark.table(customers_table).customer_id)\\n    .select(orders_bronze_streaming_view.order_id, orders_bronze_streaming_view.quantity, orders_bronze_streaming_view.customer_id, orders_bronze_streaming_view.books, F.col('profile').getItem('first_name').alias('f_name'), F.col('profile').getItem('last_name').alias('l_name'))\\n    .filter(orders_bronze_streaming_view.quantity > 0)\\n\\ncheckpoint_location = 'dbfs:/mnt/bookstore/checkpoints/orders_silver'\\n\\nquery = joined_data.writeStream.format('delta')\\n    .outputMode('append')\\n    .option('checkpointLocation', checkpoint_location)\\n    .trigger(availableNow=True)\\n    .table(orders_silver_table)\", 'part3': \"books_table = 'books'\\nsales_by_author_table = 'sales_by_author'\\n\\norders_silver_streaming = spark.readStream.format('delta').table(orders_silver_table)\\n\\nenriched_data = orders_silver_streaming.select(F.explode('books').alias('book'), 'quantity', 'f_name', 'l_name')\\n    .join(spark.table(books_table), ['book.book_id'])\\n    .groupBy(F.col('author'))\\n    .agg(F.sum('book.subtotal').alias('Total_Sales_Amount'), F.sum('book.quantity').alias('Total_Sales_Quantity'))\\n\\ncheckpoint_location = 'dbfs:/mnt/bookstore/checkpoints/sales_by_author'\\n\\nquery = enriched_data.writeStream.format('delta')\\n    .outputMode('append')\\n    .option('checkpointLocation', checkpoint_location)\\n    .trigger(availableNow=True)\\n    .table(sales_by_author_table)\"}\n",
            "{'part1': 'Read data using Autoloader, append file_name and processed_timestamp, write stream to orders_bronze table using Delta format, and specified checkpoint location.', 'part2': 'Joined orders_bronze and customers table, extracted first_name and last_name from profile JSON column, filtered rows with quantity>0, and wrote the stream to orders_silver table using Delta format.', 'part3': 'Flattened books array, joined with books table, aggregated by author to calculate Total_Sales_Amount and Total_Sales_Quantity, and wrote the stream to sales_by_author table using Delta format.'}\n"
          ]
        }
      ]
    }
  ]
}