{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1/423RtlrmPoh6e+njXfq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cooolbabu/GoogleGemini101/blob/main/AzureDatabricks/BookstoreMedallion_BookstoreDB_OpenAI_T1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package installs"
      ],
      "metadata": {
        "id": "awVI93fo_PXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai\n",
        "%pip install PyGithub"
      ],
      "metadata": {
        "id": "SUckIMaorrfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a172a15-4e53-498e-d05e-7c488e75e001"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.1-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.1\n",
            "Collecting PyGithub\n",
            "  Downloading PyGithub-2.2.0-py3-none-any.whl (350 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m350.2/350.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynacl>=1.4.0 (from PyGithub)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.31.0)\n",
            "Collecting pyjwt[crypto]>=2.4.0 (from PyGithub)\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (4.10.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.0.7)\n",
            "Collecting Deprecated (from PyGithub)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->PyGithub) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (2024.2.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->PyGithub) (1.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.21)\n",
            "Installing collected packages: pyjwt, Deprecated, pynacl, PyGithub\n",
            "  Attempting uninstall: pyjwt\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "Successfully installed Deprecated-1.2.14 PyGithub-2.2.0 pyjwt-2.8.0 pynacl-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports, get keys, get llm client and set model to variable MODEL_NAME"
      ],
      "metadata": {
        "id": "xXANfThOTUJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "import re\n",
        "from openai import OpenAI\n",
        "from pprint import pprint\n",
        "from google.colab import userdata\n",
        "from github import Github\n",
        "\n",
        "# Get the OpenAI API key from Colab secrets\n",
        "github_token=userdata.get('Github_Token')\n",
        "openai_api_key=userdata.get('OPENAI_API_KEY')\n",
        "# Initialize a GitHub instance\n",
        "g = Github(github_token)\n",
        "\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "MODEL_NAME = \"gpt-3.5-turbo-1106\""
      ],
      "metadata": {
        "id": "HgzQz8VirhAw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Github helper functions\n",
        "* read_file_as_string()\n",
        "* check_in_file(repo_name, file_path, file_content, content_tag, branch)"
      ],
      "metadata": {
        "id": "2TKZOKLeXN1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_as_string(file_path):\n",
        "    \"\"\"\n",
        "        Reads the file and return a string representation of the file contents\n",
        "\n",
        "        Parameters:\n",
        "            file_path (str): Filename including filepath\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            file_contents = file.read()\n",
        "        return file_contents\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{file_path}' not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def check_in_file(repo_name, file_path, file_content, content_tag, branch):\n",
        "    \"\"\"\n",
        "        Checks if a specific file exists in a GitHub repository and updates it with new content if it does.\n",
        "        If the file does not exist, it creates a new file with the provided content.\n",
        "\n",
        "        This function operates on a specific branch named 'test'. If updating, it will commit the changes with a given content tag as the commit message.\n",
        "        In case the file needs to be created, it will also use the content tag as the commit message for the new file.\n",
        "\n",
        "        Parameters:\n",
        "        - repo_name (str): The name of the repository, formatted as 'username/repository'.\n",
        "        - file_path (str): The path to the file within the repository. This should include the file name and its extension.\n",
        "        - file_content (str): The content to be written to the file. This is used both for updating and creating the file.\n",
        "        - content_tag (str): A message associated with the commit used for updating or creating the file.\n",
        "        - branch (str): Github branch for the code\n",
        "\n",
        "        Behavior:\n",
        "        - If the file exists at the specified path, it updates the file with `file_content`, using `content_tag` as the commit message.\n",
        "        - If the file does not exist, it creates a new file at the specified path with `file_content`, also using `content_tag` as the commit message for creation.\n",
        "        - Upon successful update or creation, prints a success message indicating the action taken.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the repository\n",
        "    repo = g.get_repo(repo_name)\n",
        "\n",
        "    try:\n",
        "        # Get the contents of the file if it exists\n",
        "        file = repo.get_contents(file_path, ref=branch)\n",
        "\n",
        "        # Update the file\n",
        "        repo.update_file(file_path, content_tag, file_content, file.sha, branch=branch)\n",
        "        print(f\"File '{file_path}' updated successfully.\")\n",
        "    except:\n",
        "        # If the file doesn't exist, create it\n",
        "        print(f\"{file_path}/{file_content} does not exist\")\n",
        "        repo.create_file(file_path, content_tag, file_content, branch=branch)\n",
        "        print(f\"File '{file_path}' created successfully.\")\n",
        "\n",
        "def create_notebook(response, system_message, instructions, filename):\n",
        "    # Extract summary, code, and explanation from the response JSON\n",
        "    summary = response[\"summary\"]\n",
        "    code = response[\"code\"]\n",
        "    explanation = response[\"explanation\"]\n",
        "\n",
        "    # Create the notebook content\n",
        "    notebook_content = f\"\"\"# Databricks notebook source\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC # Summary\n",
        "# MAGIC {summary}\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC # Code (use Databricks workspace formatter to format the code)\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "{code} U+0004\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC # Explanation\n",
        "# MAGIC {explanation}\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %md\n",
        "# MAGIC # GenAI Instructions\n",
        "# MAGIC * ## AI Role\n",
        "# MAGIC {system_message}\n",
        "\n",
        "# COMMAND ----------\n",
        "# MAGIC %md\n",
        "# MAGIC * ## Instructions (Try edit mode for visualizing table structure)\n",
        "# MAGIC {instructions}\n",
        "\"\"\"\n",
        "\n",
        "    # Write the notebook content to a file\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(notebook_content)\n",
        "\n",
        "    print(f\"Notebook '{filename}' has been created.\")\n",
        "\n",
        "    return notebook_content"
      ],
      "metadata": {
        "id": "U6zhOVp3rhDW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def convert_str_to_dict(s):\n",
        "    try:\n",
        "        d = ast.literal_eval(s)\n",
        "        if isinstance(d, dict):\n",
        "            return d\n",
        "        else:\n",
        "            raise ValueError(\"Input is not a valid dictionary string\")\n",
        "    except (ValueError, SyntaxError):\n",
        "        raise ValueError(\"Input is not a valid dictionary string\")\n",
        "\n",
        "import string\n",
        "\n",
        "def strip_control_characters_old(s):\n",
        "    # Create a translation table that maps all control characters to None\n",
        "    control_chars = dict.fromkeys(range(0x00, 0x20), ' ')\n",
        "    control_chars.update(dict.fromkeys(range(0x7f, 0xa0), ' '))\n",
        "\n",
        "    # Translate the string using the translation table\n",
        "    cleaned_str = s.translate(dict.fromkeys(control_chars, ' '))\n",
        "\n",
        "    return cleaned_str\n",
        "\n",
        "def strip_control_characters(s):\n",
        "    # Create a translation table that maps all control characters and special characters to a space ' '\n",
        "    control_chars = dict.fromkeys(range(0x00, 0x09), ' ')  # Exclude \\n, \\r, \\f\n",
        "    control_chars.update(dict.fromkeys(range(0x0B, 0x0C), ' '))\n",
        "    control_chars.update(dict.fromkeys(range(0x0E, 0x20), ' '))\n",
        "    control_chars.update(dict.fromkeys(range(0x7f, 0xa0), ' '))\n",
        "    special_chars = dict.fromkeys(map(ord, string.punctuation.replace('\\n', '').replace('\\r', '').replace('\\f', '')), ' ')\n",
        "    control_chars.update(special_chars)\n",
        "\n",
        "    # Translate the string using the translation table\n",
        "    cleaned_str = s.translate(control_chars)"
      ],
      "metadata": {
        "id": "NOGlMd278egD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "1.   System Message\n",
        "2.   User Message\n",
        "\n"
      ],
      "metadata": {
        "id": "TqsXQmSVUKDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"\n",
        "You are  Azure Databricks data engineer.\n",
        "    - You will be given tasks and asked to write pyspark code.\n",
        "    - You will use best practices for writing code.\n",
        "    - Your response will be in JSON format with keys \"summary\", \"code\", \"explanation\".\n",
        "  \"\"\".strip()\n",
        "\n",
        "user_message_content = read_file_as_string(\"./BookstorePrompt.txt\")\n",
        "print(user_message_content)"
      ],
      "metadata": {
        "id": "CXd8CCYOLCBN",
        "outputId": "7bf18fef-7e42-493b-d55c-3ca2f1de5d9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Please build a pyspark program for Azure Databricks using Medallion framework.\n",
            "I will give you the table schema. I will provide general instructions and instructions for each step. \n",
            "\n",
            "The schema for the tables is as follows\n",
            "\n",
            "customers table schema\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- profile: string (nullable = true)\n",
            " |-- updated: string (nullable = true)\n",
            "\n",
            "books table schema\n",
            "root\n",
            " |-- book_id: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- author: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " \n",
            "orders_bronze table schema\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- order_timestamp: long (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- quantity: long (nullable = true)\n",
            " |-- total: integer (nullable = true)\n",
            " |-- books: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- book_id: string (nullable = true)\n",
            " |    |    |-- quantity: integer (nullable = true)\n",
            " |    |    |-- subtotal: long (nullable = true)\n",
            " |-- _rescued_data: string (nullable = true)\n",
            " |-- file_name: string (nullable = true)\n",
            " |-- processed_timestamp: timestamp (nullable = true)\n",
            " \n",
            "\n",
            "orders_silver schema\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- quantity: long (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- books: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- book_id: string (nullable = true)\n",
            " |    |    |-- quantity: integer (nullable = true)\n",
            " |    |    |-- subtotal: long (nullable = true)\n",
            " |-- f_name: string (nullable = true)\n",
            " |-- l_name: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " \n",
            "\n",
            "sales_by_author schema\n",
            "root\n",
            " |-- author: string (nullable = true)\n",
            " |-- Total_Sales_Amount: long (nullable = true)\n",
            " |-- Total_Sales_Quantity: long (nullable = true)\n",
            "\n",
            "General Instructions:\n",
            "\n",
            "1 - Use variables following programming best practices\n",
            "2 - Root directory for the checkpoint folder is dbfs:/mnt/bookstore/checkpoints/\n",
            "3 - Root directory for the schemas folder is dbfs:/mnt/bookstore/schemas/\n",
            "4 - Define necessary sub-folders as required. \n",
            "\n",
            "The instructions are given in three parts\n",
            "Part 1: Ingesting Data into Orders_Bronze Table\n",
            "1 - input folder location for raw data is dbfs:/mnt/bookstore/orders-raw\n",
            "2 - Initialize a Spark session and use Autoloader to ingest data files. The file format is parquet.\n",
            "3 - Append file_name and processed_timestamp columns using input_file_name() and current_timestamp() functions, respectively.\n",
            "4 - Write the stream to target table orders_bronze table. Use Append as output mode. Specify checkpoint location.\n",
            "5 - use .trigger(availableNow=True)\n",
            "6 - Use toTable() function\n",
            "7 - Use Autoloader schema evolution functionality\n",
            "\n",
            "\n",
            "Part 2: Writing Orders_Silver Table from Orders_Bronze and Customers\n",
            "1 - Table names and locations are stored in variables\n",
            "1 - Read from orders_bronze table as a stream and create a temporary streaming view named orders_bronze_streaming_view.\n",
            "2 - Load the customers table\n",
            "3 - Perform a SQL join between orders_bronze_streaming_view and customers table on customer_id. Selecting order details. Extract first_name and last_name from profile column. Profile column is a json object.\n",
            "4 - select only rows that have quantity greater than 0\n",
            "5 - Write the joined data to the orders_silver table using writeStream with append mode. Specify checkpoint location.\n",
            "6 - use .trigger(availableNow=True)\n",
            "7 - Use toTable() function\n",
            "\n",
            "Part 3: Populating Sales_by_Author Table from Orders_Silver\n",
            "1 - Table names and locations are stored in variables\n",
            "2 - Read from the orders_silver table as a stream, explode the books array to flatten the book details, and select necessary columns for processing.\n",
            "3 - Assume the books table exists and contains book_id and author, perform a join to enrich book items with author.\n",
            "4 - Group by author and aggregate to calculate Total_Sales_Amount and Total_Sales_Quantity.\n",
            "5 - Write the aggregated data to the sales_by_author table using writeStream\n",
            "6 - use .trigger(availableNow=True) \n",
            "7 - Use toTable() function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make the call to LLMs"
      ],
      "metadata": {
        "id": "_MDSbrT9UpOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the message with variables\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    response_format = {\"type\" : \"json_object\"},\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message_content}]\n",
        ")\n",
        "\n",
        "# Assuming you have a client setup for interaction. Ensure to configure your OpenAI client appropriately.\n",
        "\n"
      ],
      "metadata": {
        "id": "DAKT9wxMrhGK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nResponse id: {response.id}\\nCreated: {response.created}\\nModel: {response.model}\\nCompletion Tokens: {response.usage.completion_tokens}\\nPrompt Tokens: {response.usage.prompt_tokens}\\nTotal tokens: {response.usage.total_tokens}\")\n",
        "#print(response.created)\n"
      ],
      "metadata": {
        "id": "hYXOwUpo-MqT",
        "outputId": "e763c855-4cb8-4bdd-a38a-695b496ba5c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response id: chatcmpl-93UWtqKT3jxXQMlW8R7aRLUDwQZII\n",
            "Created: 1710619367\n",
            "Model: gpt-3.5-turbo-0125\n",
            "Completion Tokens: 931\n",
            "Prompt Tokens: 1026\n",
            "Total tokens: 1957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "pprint.pprint(response.dict())"
      ],
      "metadata": {
        "id": "Oh8Kttk-_bOT",
        "outputId": "08f01c07-ac7e-4868-bf86-68981b0beae0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'choices': [{'finish_reason': 'stop',\n",
            "              'index': 0,\n",
            "              'logprobs': None,\n",
            "              'message': {'content': '{\\n'\n",
            "                                     '    \"summary\": \"Created a Pyspark '\n",
            "                                     'program using Medallion framework for '\n",
            "                                     'Azure Databricks following the '\n",
            "                                     'instructions provided.\",\\n'\n",
            "                                     '    \"code\": {\\n'\n",
            "                                     '        \"part1\": {\\n'\n",
            "                                     '            \"ingest_orders_bronze\": {\\n'\n",
            "                                     '                \"code\": \"spark = '\n",
            "                                     \"SparkSession.builder.appName('OrderBronzeIngest').getOrCreate()\\\\n\\\\norders_bronze_df \"\n",
            "                                     '= '\n",
            "                                     \"spark.readStream.format('cloudFiles')\\\\n    \"\n",
            "                                     \".option('cloudFiles.format', \"\n",
            "                                     \"'parquet')\\\\n    \"\n",
            "                                     \".option('cloudFiles.includeExistingFiles', \"\n",
            "                                     \"'true')\\\\n    \"\n",
            "                                     \".option('cloudFiles.useNotifications', \"\n",
            "                                     \"'true')\\\\n    .option('cloudFiles.url', \"\n",
            "                                     \"'dbfs:/mnt/bookstore/orders-raw')\\\\n    \"\n",
            "                                     \".option('cloudFiles.format', \"\n",
            "                                     \"'parquet')\\\\n    \"\n",
            "                                     '.load()\\\\n\\\\norders_bronze_df = '\n",
            "                                     \"orders_bronze_df.withColumn('file_name', \"\n",
            "                                     'input_file_name())\\\\n    '\n",
            "                                     \".withColumn('processed_timestamp', \"\n",
            "                                     'current_timestamp())\\\\n\\\\ncheckpoint_location '\n",
            "                                     '= '\n",
            "                                     \"'dbfs:/mnt/bookstore/checkpoints/orders_bronze'\\\\n\\\\nquery \"\n",
            "                                     '= '\n",
            "                                     \"orders_bronze_df.writeStream.format('delta')\\\\n    \"\n",
            "                                     \".outputMode('append')\\\\n    \"\n",
            "                                     \".option('checkpointLocation', \"\n",
            "                                     'checkpoint_location)\\\\n    '\n",
            "                                     '.trigger(availableNow=True)\\\\n    '\n",
            "                                     '.table(\\'orders_bronze\\')\",\\n'\n",
            "                                     '                \"explanation\": \"Read '\n",
            "                                     'data using Autoloader, append file_name '\n",
            "                                     'and processed_timestamp, write stream to '\n",
            "                                     'orders_bronze table using Delta format, '\n",
            "                                     'and specified checkpoint location.\"\\n'\n",
            "                                     '            }\\n'\n",
            "                                     '        },\\n'\n",
            "                                     '        \"part2\": {\\n'\n",
            "                                     '            \"write_orders_silver\": {\\n'\n",
            "                                     '                \"code\": \"customers_table '\n",
            "                                     \"= 'customers'\\\\norders_bronze_table = \"\n",
            "                                     \"'orders_bronze'\\\\norders_silver_table = \"\n",
            "                                     \"'orders_silver'\\\\n\\\\norders_bronze_streaming_view \"\n",
            "                                     '= '\n",
            "                                     \"spark.readStream.format('delta').table(orders_bronze_table)\\\\n\\\\njoined_data \"\n",
            "                                     '= '\n",
            "                                     'orders_bronze_streaming_view.join(F.broadcast(spark.table(customers_table)),\\\\n    '\n",
            "                                     'orders_bronze_streaming_view.customer_id '\n",
            "                                     '== '\n",
            "                                     'spark.table(customers_table).customer_id)\\\\n    '\n",
            "                                     '.select(orders_bronze_streaming_view.order_id, '\n",
            "                                     'orders_bronze_streaming_view.quantity, '\n",
            "                                     'orders_bronze_streaming_view.customer_id, '\n",
            "                                     'orders_bronze_streaming_view.books, '\n",
            "                                     \"F.col('profile').getItem('first_name').alias('f_name'), \"\n",
            "                                     \"F.col('profile').getItem('last_name').alias('l_name'))\\\\n    \"\n",
            "                                     '.filter(orders_bronze_streaming_view.quantity '\n",
            "                                     '> 0)\\\\n\\\\ncheckpoint_location = '\n",
            "                                     \"'dbfs:/mnt/bookstore/checkpoints/orders_silver'\\\\n\\\\nquery \"\n",
            "                                     '= '\n",
            "                                     \"joined_data.writeStream.format('delta')\\\\n    \"\n",
            "                                     \".outputMode('append')\\\\n    \"\n",
            "                                     \".option('checkpointLocation', \"\n",
            "                                     'checkpoint_location)\\\\n    '\n",
            "                                     '.trigger(availableNow=True)\\\\n    '\n",
            "                                     '.table(orders_silver_table)\",\\n'\n",
            "                                     '                \"explanation\": \"Joined '\n",
            "                                     'orders_bronze and customers table, '\n",
            "                                     'extracted first_name and last_name from '\n",
            "                                     'profile JSON column, filtered rows with '\n",
            "                                     'quantity>0, and wrote the stream to '\n",
            "                                     'orders_silver table using Delta '\n",
            "                                     'format.\"\\n'\n",
            "                                     '            }\\n'\n",
            "                                     '        },\\n'\n",
            "                                     '        \"part3\": {\\n'\n",
            "                                     '            \"populate_sales_by_author\": '\n",
            "                                     '{\\n'\n",
            "                                     '                \"code\": \"books_table = '\n",
            "                                     \"'books'\\\\nsales_by_author_table = \"\n",
            "                                     \"'sales_by_author'\\\\n\\\\norders_silver_streaming \"\n",
            "                                     '= '\n",
            "                                     \"spark.readStream.format('delta').table(orders_silver_table)\\\\n\\\\nenriched_data \"\n",
            "                                     '= '\n",
            "                                     \"orders_silver_streaming.select(F.explode('books').alias('book'), \"\n",
            "                                     \"'quantity', 'f_name', 'l_name')\\\\n    \"\n",
            "                                     '.join(spark.table(books_table), '\n",
            "                                     \"['book.book_id'])\\\\n    \"\n",
            "                                     \".groupBy(F.col('author'))\\\\n    \"\n",
            "                                     \".agg(F.sum('book.subtotal').alias('Total_Sales_Amount'), \"\n",
            "                                     \"F.sum('book.quantity').alias('Total_Sales_Quantity'))\\\\n\\\\ncheckpoint_location \"\n",
            "                                     '= '\n",
            "                                     \"'dbfs:/mnt/bookstore/checkpoints/sales_by_author'\\\\n\\\\nquery \"\n",
            "                                     '= '\n",
            "                                     \"enriched_data.writeStream.format('delta')\\\\n    \"\n",
            "                                     \".outputMode('append')\\\\n    \"\n",
            "                                     \".option('checkpointLocation', \"\n",
            "                                     'checkpoint_location)\\\\n    '\n",
            "                                     '.trigger(availableNow=True)\\\\n    '\n",
            "                                     '.table(sales_by_author_table)\",\\n'\n",
            "                                     '                \"explanation\": '\n",
            "                                     '\"Flattened books array, joined with '\n",
            "                                     'books table, aggregated by author to '\n",
            "                                     'calculate Total_Sales_Amount and '\n",
            "                                     'Total_Sales_Quantity, and wrote the '\n",
            "                                     'stream to sales_by_author table using '\n",
            "                                     'Delta format.\"\\n'\n",
            "                                     '            }\\n'\n",
            "                                     '        }\\n'\n",
            "                                     '    }\\n'\n",
            "                                     '}',\n",
            "                          'function_call': None,\n",
            "                          'role': 'assistant',\n",
            "                          'tool_calls': None}}],\n",
            " 'created': 1710619367,\n",
            " 'id': 'chatcmpl-93UWtqKT3jxXQMlW8R7aRLUDwQZII',\n",
            " 'model': 'gpt-3.5-turbo-0125',\n",
            " 'object': 'chat.completion',\n",
            " 'system_fingerprint': 'fp_4f2ebda25a',\n",
            " 'usage': {'completion_tokens': 931,\n",
            "           'prompt_tokens': 1026,\n",
            "           'total_tokens': 1957}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Response id: {response.id}\")\n",
        "json_str = json.dumps(response.json(), indent=4, sort_keys=True)\n",
        "print(json_str)"
      ],
      "metadata": {
        "id": "7mi4K9XL3PjF",
        "outputId": "906e55fb-7248-41d7-fbfb-dd13713a8171",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response id: chatcmpl-93UWtqKT3jxXQMlW8R7aRLUDwQZII\n",
            "\"{\\\"id\\\":\\\"chatcmpl-93UWtqKT3jxXQMlW8R7aRLUDwQZII\\\",\\\"choices\\\":[{\\\"finish_reason\\\":\\\"stop\\\",\\\"index\\\":0,\\\"logprobs\\\":null,\\\"message\\\":{\\\"content\\\":\\\"{\\\\n    \\\\\\\"summary\\\\\\\": \\\\\\\"Created a Pyspark program using Medallion framework for Azure Databricks following the instructions provided.\\\\\\\",\\\\n    \\\\\\\"code\\\\\\\": {\\\\n        \\\\\\\"part1\\\\\\\": {\\\\n            \\\\\\\"ingest_orders_bronze\\\\\\\": {\\\\n                \\\\\\\"code\\\\\\\": \\\\\\\"spark = SparkSession.builder.appName('OrderBronzeIngest').getOrCreate()\\\\\\\\n\\\\\\\\norders_bronze_df = spark.readStream.format('cloudFiles')\\\\\\\\n    .option('cloudFiles.format', 'parquet')\\\\\\\\n    .option('cloudFiles.includeExistingFiles', 'true')\\\\\\\\n    .option('cloudFiles.useNotifications', 'true')\\\\\\\\n    .option('cloudFiles.url', 'dbfs:/mnt/bookstore/orders-raw')\\\\\\\\n    .option('cloudFiles.format', 'parquet')\\\\\\\\n    .load()\\\\\\\\n\\\\\\\\norders_bronze_df = orders_bronze_df.withColumn('file_name', input_file_name())\\\\\\\\n    .withColumn('processed_timestamp', current_timestamp())\\\\\\\\n\\\\\\\\ncheckpoint_location = 'dbfs:/mnt/bookstore/checkpoints/orders_bronze'\\\\\\\\n\\\\\\\\nquery = orders_bronze_df.writeStream.format('delta')\\\\\\\\n    .outputMode('append')\\\\\\\\n    .option('checkpointLocation', checkpoint_location)\\\\\\\\n    .trigger(availableNow=True)\\\\\\\\n    .table('orders_bronze')\\\\\\\",\\\\n                \\\\\\\"explanation\\\\\\\": \\\\\\\"Read data using Autoloader, append file_name and processed_timestamp, write stream to orders_bronze table using Delta format, and specified checkpoint location.\\\\\\\"\\\\n            }\\\\n        },\\\\n        \\\\\\\"part2\\\\\\\": {\\\\n            \\\\\\\"write_orders_silver\\\\\\\": {\\\\n                \\\\\\\"code\\\\\\\": \\\\\\\"customers_table = 'customers'\\\\\\\\norders_bronze_table = 'orders_bronze'\\\\\\\\norders_silver_table = 'orders_silver'\\\\\\\\n\\\\\\\\norders_bronze_streaming_view = spark.readStream.format('delta').table(orders_bronze_table)\\\\\\\\n\\\\\\\\njoined_data = orders_bronze_streaming_view.join(F.broadcast(spark.table(customers_table)),\\\\\\\\n    orders_bronze_streaming_view.customer_id == spark.table(customers_table).customer_id)\\\\\\\\n    .select(orders_bronze_streaming_view.order_id, orders_bronze_streaming_view.quantity, orders_bronze_streaming_view.customer_id, orders_bronze_streaming_view.books, F.col('profile').getItem('first_name').alias('f_name'), F.col('profile').getItem('last_name').alias('l_name'))\\\\\\\\n    .filter(orders_bronze_streaming_view.quantity > 0)\\\\\\\\n\\\\\\\\ncheckpoint_location = 'dbfs:/mnt/bookstore/checkpoints/orders_silver'\\\\\\\\n\\\\\\\\nquery = joined_data.writeStream.format('delta')\\\\\\\\n    .outputMode('append')\\\\\\\\n    .option('checkpointLocation', checkpoint_location)\\\\\\\\n    .trigger(availableNow=True)\\\\\\\\n    .table(orders_silver_table)\\\\\\\",\\\\n                \\\\\\\"explanation\\\\\\\": \\\\\\\"Joined orders_bronze and customers table, extracted first_name and last_name from profile JSON column, filtered rows with quantity>0, and wrote the stream to orders_silver table using Delta format.\\\\\\\"\\\\n            }\\\\n        },\\\\n        \\\\\\\"part3\\\\\\\": {\\\\n            \\\\\\\"populate_sales_by_author\\\\\\\": {\\\\n                \\\\\\\"code\\\\\\\": \\\\\\\"books_table = 'books'\\\\\\\\nsales_by_author_table = 'sales_by_author'\\\\\\\\n\\\\\\\\norders_silver_streaming = spark.readStream.format('delta').table(orders_silver_table)\\\\\\\\n\\\\\\\\nenriched_data = orders_silver_streaming.select(F.explode('books').alias('book'), 'quantity', 'f_name', 'l_name')\\\\\\\\n    .join(spark.table(books_table), ['book.book_id'])\\\\\\\\n    .groupBy(F.col('author'))\\\\\\\\n    .agg(F.sum('book.subtotal').alias('Total_Sales_Amount'), F.sum('book.quantity').alias('Total_Sales_Quantity'))\\\\\\\\n\\\\\\\\ncheckpoint_location = 'dbfs:/mnt/bookstore/checkpoints/sales_by_author'\\\\\\\\n\\\\\\\\nquery = enriched_data.writeStream.format('delta')\\\\\\\\n    .outputMode('append')\\\\\\\\n    .option('checkpointLocation', checkpoint_location)\\\\\\\\n    .trigger(availableNow=True)\\\\\\\\n    .table(sales_by_author_table)\\\\\\\",\\\\n                \\\\\\\"explanation\\\\\\\": \\\\\\\"Flattened books array, joined with books table, aggregated by author to calculate Total_Sales_Amount and Total_Sales_Quantity, and wrote the stream to sales_by_author table using Delta format.\\\\\\\"\\\\n            }\\\\n        }\\\\n    }\\\\n}\\\",\\\"role\\\":\\\"assistant\\\",\\\"function_call\\\":null,\\\"tool_calls\\\":null}}],\\\"created\\\":1710619367,\\\"model\\\":\\\"gpt-3.5-turbo-0125\\\",\\\"object\\\":\\\"chat.completion\\\",\\\"system_fingerprint\\\":\\\"fp_4f2ebda25a\\\",\\\"usage\\\":{\\\"completion_tokens\\\":931,\\\"prompt_tokens\\\":1026,\\\"total_tokens\\\":1957}}\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(response.choices[0])"
      ],
      "metadata": {
        "id": "qHbV2EzkL3qO",
        "outputId": "f38ed0d8-1a53-4042-e299-87428d99299a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n    \"summary\": \"Created a Pyspark program using Medallion framework for Azure Databricks following the instructions provided.\",\\n    \"code\": {\\n        \"part1\": {\\n            \"ingest_orders_bronze\": {\\n                \"code\": \"spark = SparkSession.builder.appName(\\'OrderBronzeIngest\\').getOrCreate()\\\\n\\\\norders_bronze_df = spark.readStream.format(\\'cloudFiles\\')\\\\n    .option(\\'cloudFiles.format\\', \\'parquet\\')\\\\n    .option(\\'cloudFiles.includeExistingFiles\\', \\'true\\')\\\\n    .option(\\'cloudFiles.useNotifications\\', \\'true\\')\\\\n    .option(\\'cloudFiles.url\\', \\'dbfs:/mnt/bookstore/orders-raw\\')\\\\n    .option(\\'cloudFiles.format\\', \\'parquet\\')\\\\n    .load()\\\\n\\\\norders_bronze_df = orders_bronze_df.withColumn(\\'file_name\\', input_file_name())\\\\n    .withColumn(\\'processed_timestamp\\', current_timestamp())\\\\n\\\\ncheckpoint_location = \\'dbfs:/mnt/bookstore/checkpoints/orders_bronze\\'\\\\n\\\\nquery = orders_bronze_df.writeStream.format(\\'delta\\')\\\\n    .outputMode(\\'append\\')\\\\n    .option(\\'checkpointLocation\\', checkpoint_location)\\\\n    .trigger(availableNow=True)\\\\n    .table(\\'orders_bronze\\')\",\\n                \"explanation\": \"Read data using Autoloader, append file_name and processed_timestamp, write stream to orders_bronze table using Delta format, and specified checkpoint location.\"\\n            }\\n        },\\n        \"part2\": {\\n            \"write_orders_silver\": {\\n                \"code\": \"customers_table = \\'customers\\'\\\\norders_bronze_table = \\'orders_bronze\\'\\\\norders_silver_table = \\'orders_silver\\'\\\\n\\\\norders_bronze_streaming_view = spark.readStream.format(\\'delta\\').table(orders_bronze_table)\\\\n\\\\njoined_data = orders_bronze_streaming_view.join(F.broadcast(spark.table(customers_table)),\\\\n    orders_bronze_streaming_view.customer_id == spark.table(customers_table).customer_id)\\\\n    .select(orders_bronze_streaming_view.order_id, orders_bronze_streaming_view.quantity, orders_bronze_streaming_view.customer_id, orders_bronze_streaming_view.books, F.col(\\'profile\\').getItem(\\'first_name\\').alias(\\'f_name\\'), F.col(\\'profile\\').getItem(\\'last_name\\').alias(\\'l_name\\'))\\\\n    .filter(orders_bronze_streaming_view.quantity > 0)\\\\n\\\\ncheckpoint_location = \\'dbfs:/mnt/bookstore/checkpoints/orders_silver\\'\\\\n\\\\nquery = joined_data.writeStream.format(\\'delta\\')\\\\n    .outputMode(\\'append\\')\\\\n    .option(\\'checkpointLocation\\', checkpoint_location)\\\\n    .trigger(availableNow=True)\\\\n    .table(orders_silver_table)\",\\n                \"explanation\": \"Joined orders_bronze and customers table, extracted first_name and last_name from profile JSON column, filtered rows with quantity>0, and wrote the stream to orders_silver table using Delta format.\"\\n            }\\n        },\\n        \"part3\": {\\n            \"populate_sales_by_author\": {\\n                \"code\": \"books_table = \\'books\\'\\\\nsales_by_author_table = \\'sales_by_author\\'\\\\n\\\\norders_silver_streaming = spark.readStream.format(\\'delta\\').table(orders_silver_table)\\\\n\\\\nenriched_data = orders_silver_streaming.select(F.explode(\\'books\\').alias(\\'book\\'), \\'quantity\\', \\'f_name\\', \\'l_name\\')\\\\n    .join(spark.table(books_table), [\\'book.book_id\\'])\\\\n    .groupBy(F.col(\\'author\\'))\\\\n    .agg(F.sum(\\'book.subtotal\\').alias(\\'Total_Sales_Amount\\'), F.sum(\\'book.quantity\\').alias(\\'Total_Sales_Quantity\\'))\\\\n\\\\ncheckpoint_location = \\'dbfs:/mnt/bookstore/checkpoints/sales_by_author\\'\\\\n\\\\nquery = enriched_data.writeStream.format(\\'delta\\')\\\\n    .outputMode(\\'append\\')\\\\n    .option(\\'checkpointLocation\\', checkpoint_location)\\\\n    .trigger(availableNow=True)\\\\n    .table(sales_by_author_table)\",\\n                \"explanation\": \"Flattened books array, joined with books table, aggregated by author to calculate Total_Sales_Amount and Total_Sales_Quantity, and wrote the stream to sales_by_author table using Delta format.\"\\n            }\\n        }\\n    }\\n}', role='assistant', function_call=None, tool_calls=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(response.choices[0].message)"
      ],
      "metadata": {
        "id": "jNUavIntPBIc",
        "outputId": "37ff47f7-bd2c-4a42-bd6e-fc08924ec7be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content='{\\n    \"summary\": \"Created a Pyspark program using Medallion framework for Azure Databricks following the instructions provided.\",\\n    \"code\": {\\n        \"part1\": {\\n            \"ingest_orders_bronze\": {\\n                \"code\": \"spark = SparkSession.builder.appName(\\'OrderBronzeIngest\\').getOrCreate()\\\\n\\\\norders_bronze_df = spark.readStream.format(\\'cloudFiles\\')\\\\n    .option(\\'cloudFiles.format\\', \\'parquet\\')\\\\n    .option(\\'cloudFiles.includeExistingFiles\\', \\'true\\')\\\\n    .option(\\'cloudFiles.useNotifications\\', \\'true\\')\\\\n    .option(\\'cloudFiles.url\\', \\'dbfs:/mnt/bookstore/orders-raw\\')\\\\n    .option(\\'cloudFiles.format\\', \\'parquet\\')\\\\n    .load()\\\\n\\\\norders_bronze_df = orders_bronze_df.withColumn(\\'file_name\\', input_file_name())\\\\n    .withColumn(\\'processed_timestamp\\', current_timestamp())\\\\n\\\\ncheckpoint_location = \\'dbfs:/mnt/bookstore/checkpoints/orders_bronze\\'\\\\n\\\\nquery = orders_bronze_df.writeStream.format(\\'delta\\')\\\\n    .outputMode(\\'append\\')\\\\n    .option(\\'checkpointLocation\\', checkpoint_location)\\\\n    .trigger(availableNow=True)\\\\n    .table(\\'orders_bronze\\')\",\\n                \"explanation\": \"Read data using Autoloader, append file_name and processed_timestamp, write stream to orders_bronze table using Delta format, and specified checkpoint location.\"\\n            }\\n        },\\n        \"part2\": {\\n            \"write_orders_silver\": {\\n                \"code\": \"customers_table = \\'customers\\'\\\\norders_bronze_table = \\'orders_bronze\\'\\\\norders_silver_table = \\'orders_silver\\'\\\\n\\\\norders_bronze_streaming_view = spark.readStream.format(\\'delta\\').table(orders_bronze_table)\\\\n\\\\njoined_data = orders_bronze_streaming_view.join(F.broadcast(spark.table(customers_table)),\\\\n    orders_bronze_streaming_view.customer_id == spark.table(customers_table).customer_id)\\\\n    .select(orders_bronze_streaming_view.order_id, orders_bronze_streaming_view.quantity, orders_bronze_streaming_view.customer_id, orders_bronze_streaming_view.books, F.col(\\'profile\\').getItem(\\'first_name\\').alias(\\'f_name\\'), F.col(\\'profile\\').getItem(\\'last_name\\').alias(\\'l_name\\'))\\\\n    .filter(orders_bronze_streaming_view.quantity > 0)\\\\n\\\\ncheckpoint_location = \\'dbfs:/mnt/bookstore/checkpoints/orders_silver\\'\\\\n\\\\nquery = joined_data.writeStream.format(\\'delta\\')\\\\n    .outputMode(\\'append\\')\\\\n    .option(\\'checkpointLocation\\', checkpoint_location)\\\\n    .trigger(availableNow=True)\\\\n    .table(orders_silver_table)\",\\n                \"explanation\": \"Joined orders_bronze and customers table, extracted first_name and last_name from profile JSON column, filtered rows with quantity>0, and wrote the stream to orders_silver table using Delta format.\"\\n            }\\n        },\\n        \"part3\": {\\n            \"populate_sales_by_author\": {\\n                \"code\": \"books_table = \\'books\\'\\\\nsales_by_author_table = \\'sales_by_author\\'\\\\n\\\\norders_silver_streaming = spark.readStream.format(\\'delta\\').table(orders_silver_table)\\\\n\\\\nenriched_data = orders_silver_streaming.select(F.explode(\\'books\\').alias(\\'book\\'), \\'quantity\\', \\'f_name\\', \\'l_name\\')\\\\n    .join(spark.table(books_table), [\\'book.book_id\\'])\\\\n    .groupBy(F.col(\\'author\\'))\\\\n    .agg(F.sum(\\'book.subtotal\\').alias(\\'Total_Sales_Amount\\'), F.sum(\\'book.quantity\\').alias(\\'Total_Sales_Quantity\\'))\\\\n\\\\ncheckpoint_location = \\'dbfs:/mnt/bookstore/checkpoints/sales_by_author\\'\\\\n\\\\nquery = enriched_data.writeStream.format(\\'delta\\')\\\\n    .outputMode(\\'append\\')\\\\n    .option(\\'checkpointLocation\\', checkpoint_location)\\\\n    .trigger(availableNow=True)\\\\n    .table(sales_by_author_table)\",\\n                \"explanation\": \"Flattened books array, joined with books table, aggregated by author to calculate Total_Sales_Amount and Total_Sales_Quantity, and wrote the stream to sales_by_author table using Delta format.\"\\n            }\\n        }\\n    }\\n}', role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content_dict = json.loads(response.choices[0].message.content)\n",
        "print(content_dict[\"summary\"])\n",
        "print(content_dict[\"code\"])\n",
        "print(content_dict[\"code\"][\"part1\"][\"ingest_orders_bronze\"][\"code\"])\n",
        "print(content_dict[\"code\"][\"part1\"][\"ingest_orders_bronze\"][\"explanation\"])\n",
        "print(content_dict[\"code\"][\"part2\"])\n",
        "print(content_dict[\"code\"][\"part3\"])"
      ],
      "metadata": {
        "id": "13OH-Av_7lHA",
        "outputId": "190115fe-7ae3-403a-af18-dceacb301143",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created a Pyspark program using Medallion framework for Azure Databricks following the instructions provided.\n",
            "{'part1': {'ingest_orders_bronze': {'code': \"spark = SparkSession.builder.appName('OrderBronzeIngest').getOrCreate()\\n\\norders_bronze_df = spark.readStream.format('cloudFiles')\\n    .option('cloudFiles.format', 'parquet')\\n    .option('cloudFiles.includeExistingFiles', 'true')\\n    .option('cloudFiles.useNotifications', 'true')\\n    .option('cloudFiles.url', 'dbfs:/mnt/bookstore/orders-raw')\\n    .option('cloudFiles.format', 'parquet')\\n    .load()\\n\\norders_bronze_df = orders_bronze_df.withColumn('file_name', input_file_name())\\n    .withColumn('processed_timestamp', current_timestamp())\\n\\ncheckpoint_location = 'dbfs:/mnt/bookstore/checkpoints/orders_bronze'\\n\\nquery = orders_bronze_df.writeStream.format('delta')\\n    .outputMode('append')\\n    .option('checkpointLocation', checkpoint_location)\\n    .trigger(availableNow=True)\\n    .table('orders_bronze')\", 'explanation': 'Read data using Autoloader, append file_name and processed_timestamp, write stream to orders_bronze table using Delta format, and specified checkpoint location.'}}, 'part2': {'write_orders_silver': {'code': \"customers_table = 'customers'\\norders_bronze_table = 'orders_bronze'\\norders_silver_table = 'orders_silver'\\n\\norders_bronze_streaming_view = spark.readStream.format('delta').table(orders_bronze_table)\\n\\njoined_data = orders_bronze_streaming_view.join(F.broadcast(spark.table(customers_table)),\\n    orders_bronze_streaming_view.customer_id == spark.table(customers_table).customer_id)\\n    .select(orders_bronze_streaming_view.order_id, orders_bronze_streaming_view.quantity, orders_bronze_streaming_view.customer_id, orders_bronze_streaming_view.books, F.col('profile').getItem('first_name').alias('f_name'), F.col('profile').getItem('last_name').alias('l_name'))\\n    .filter(orders_bronze_streaming_view.quantity > 0)\\n\\ncheckpoint_location = 'dbfs:/mnt/bookstore/checkpoints/orders_silver'\\n\\nquery = joined_data.writeStream.format('delta')\\n    .outputMode('append')\\n    .option('checkpointLocation', checkpoint_location)\\n    .trigger(availableNow=True)\\n    .table(orders_silver_table)\", 'explanation': 'Joined orders_bronze and customers table, extracted first_name and last_name from profile JSON column, filtered rows with quantity>0, and wrote the stream to orders_silver table using Delta format.'}}, 'part3': {'populate_sales_by_author': {'code': \"books_table = 'books'\\nsales_by_author_table = 'sales_by_author'\\n\\norders_silver_streaming = spark.readStream.format('delta').table(orders_silver_table)\\n\\nenriched_data = orders_silver_streaming.select(F.explode('books').alias('book'), 'quantity', 'f_name', 'l_name')\\n    .join(spark.table(books_table), ['book.book_id'])\\n    .groupBy(F.col('author'))\\n    .agg(F.sum('book.subtotal').alias('Total_Sales_Amount'), F.sum('book.quantity').alias('Total_Sales_Quantity'))\\n\\ncheckpoint_location = 'dbfs:/mnt/bookstore/checkpoints/sales_by_author'\\n\\nquery = enriched_data.writeStream.format('delta')\\n    .outputMode('append')\\n    .option('checkpointLocation', checkpoint_location)\\n    .trigger(availableNow=True)\\n    .table(sales_by_author_table)\", 'explanation': 'Flattened books array, joined with books table, aggregated by author to calculate Total_Sales_Amount and Total_Sales_Quantity, and wrote the stream to sales_by_author table using Delta format.'}}}\n",
            "spark = SparkSession.builder.appName('OrderBronzeIngest').getOrCreate()\n",
            "\n",
            "orders_bronze_df = spark.readStream.format('cloudFiles')\n",
            "    .option('cloudFiles.format', 'parquet')\n",
            "    .option('cloudFiles.includeExistingFiles', 'true')\n",
            "    .option('cloudFiles.useNotifications', 'true')\n",
            "    .option('cloudFiles.url', 'dbfs:/mnt/bookstore/orders-raw')\n",
            "    .option('cloudFiles.format', 'parquet')\n",
            "    .load()\n",
            "\n",
            "orders_bronze_df = orders_bronze_df.withColumn('file_name', input_file_name())\n",
            "    .withColumn('processed_timestamp', current_timestamp())\n",
            "\n",
            "checkpoint_location = 'dbfs:/mnt/bookstore/checkpoints/orders_bronze'\n",
            "\n",
            "query = orders_bronze_df.writeStream.format('delta')\n",
            "    .outputMode('append')\n",
            "    .option('checkpointLocation', checkpoint_location)\n",
            "    .trigger(availableNow=True)\n",
            "    .table('orders_bronze')\n",
            "Read data using Autoloader, append file_name and processed_timestamp, write stream to orders_bronze table using Delta format, and specified checkpoint location.\n",
            "{'write_orders_silver': {'code': \"customers_table = 'customers'\\norders_bronze_table = 'orders_bronze'\\norders_silver_table = 'orders_silver'\\n\\norders_bronze_streaming_view = spark.readStream.format('delta').table(orders_bronze_table)\\n\\njoined_data = orders_bronze_streaming_view.join(F.broadcast(spark.table(customers_table)),\\n    orders_bronze_streaming_view.customer_id == spark.table(customers_table).customer_id)\\n    .select(orders_bronze_streaming_view.order_id, orders_bronze_streaming_view.quantity, orders_bronze_streaming_view.customer_id, orders_bronze_streaming_view.books, F.col('profile').getItem('first_name').alias('f_name'), F.col('profile').getItem('last_name').alias('l_name'))\\n    .filter(orders_bronze_streaming_view.quantity > 0)\\n\\ncheckpoint_location = 'dbfs:/mnt/bookstore/checkpoints/orders_silver'\\n\\nquery = joined_data.writeStream.format('delta')\\n    .outputMode('append')\\n    .option('checkpointLocation', checkpoint_location)\\n    .trigger(availableNow=True)\\n    .table(orders_silver_table)\", 'explanation': 'Joined orders_bronze and customers table, extracted first_name and last_name from profile JSON column, filtered rows with quantity>0, and wrote the stream to orders_silver table using Delta format.'}}\n",
            "{'populate_sales_by_author': {'code': \"books_table = 'books'\\nsales_by_author_table = 'sales_by_author'\\n\\norders_silver_streaming = spark.readStream.format('delta').table(orders_silver_table)\\n\\nenriched_data = orders_silver_streaming.select(F.explode('books').alias('book'), 'quantity', 'f_name', 'l_name')\\n    .join(spark.table(books_table), ['book.book_id'])\\n    .groupBy(F.col('author'))\\n    .agg(F.sum('book.subtotal').alias('Total_Sales_Amount'), F.sum('book.quantity').alias('Total_Sales_Quantity'))\\n\\ncheckpoint_location = 'dbfs:/mnt/bookstore/checkpoints/sales_by_author'\\n\\nquery = enriched_data.writeStream.format('delta')\\n    .outputMode('append')\\n    .option('checkpointLocation', checkpoint_location)\\n    .trigger(availableNow=True)\\n    .table(sales_by_author_table)\", 'explanation': 'Flattened books array, joined with books table, aggregated by author to calculate Total_Sales_Amount and Total_Sales_Quantity, and wrote the stream to sales_by_author table using Delta format.'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate response from LLM"
      ],
      "metadata": {
        "id": "MOTbodyRUt55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pMjSEaIcLrNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(message)\n"
      ],
      "metadata": {
        "id": "Eiv8FviXrhNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_contents = create_notebook(json.loads(stripped_message), system_message, user_message_content, \"orders_bronze_notebook-t2.py\")"
      ],
      "metadata": {
        "id": "F3ZTQpLg3RiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(file_contents)"
      ],
      "metadata": {
        "id": "wjTxx8GwZV6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check into Github\n",
        "*   repository : \"cooolbabu/GoogleGemini101\"\n",
        "*   filename : \"AzureDatabricks/filename\" - specify actual filename\n",
        "*   filecontent: Contents of the file to check in\n",
        "*   tag_name: give a comment. It will show in Github\n",
        "* branch: branch name to check into. Ensure that branch already exists\n",
        "          Future TODO: if branch doesn't exist (notify, ask, process)\n",
        "\n"
      ],
      "metadata": {
        "id": "zn-veyM-U19x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "check_in_file(repo_name=\"cooolbabu/GoogleGemini101\",\n",
        "              file_path=\"AzureDatabricks/ConfigureDB/create_order_table-t1.py\",\n",
        "              file_content=file_contents,\n",
        "              content_tag='creating orders table added control characters t5',\n",
        "              branch=\"pyspark-genai-t2\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ELMDUfXRBi7V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}